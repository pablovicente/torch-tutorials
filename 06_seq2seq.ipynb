{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6402e86-8a4d-450f-95de-d4fde81cf454",
   "metadata": {},
   "source": [
    "### Seq2Seq for Neural Machine Tranlation (NMT)\n",
    "\n",
    "Heavily based on PyTorch's tutorial on torchtext and github repo bentrevett/pytorch-seq2seq:\n",
    "- https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
    "- https://github.com/bentrevett/pytorch-seq2seq/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c7a78-4c2f-4118-b5b5-094a0de3f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Tuple\n",
    "from collections import Counter\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.utils import download_from_url, extract_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452abe8-c141-4b34-aa54-f0203c2b86a0",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Download French and English sentence pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166576f-0693-4f02-974c-a2d86395f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
    "train_urls = (\"train.fr.gz\", \"train.en.gz\")\n",
    "val_urls = (\"val.fr.gz\", \"val.en.gz\")\n",
    "test_urls = (\"test_2016_flickr.fr.gz\", \"test_2016_flickr.en.gz\")\n",
    "\n",
    "train_filepaths = [\n",
    "    extract_archive(download_from_url(url_base + url))[0] for url in train_urls\n",
    "]\n",
    "val_filepaths = [\n",
    "    extract_archive(download_from_url(url_base + url))[0] for url in val_urls\n",
    "]\n",
    "test_filepaths = [\n",
    "    extract_archive(download_from_url(url_base + url))[0] for url in test_urls\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a092673-ce0d-41f3-af48-ad8c563c4529",
   "metadata": {},
   "source": [
    "Samples sentences for both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f4d787-488c-4a46-96a4-1453d360e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_filepaths[0], \"r\") as f:\n",
    "    out = f.readlines()\n",
    "\n",
    "print(\"-- French --\")\n",
    "print(out[:5])\n",
    "\n",
    "with open(train_filepaths[1], \"r\") as f:\n",
    "    out = f.readlines()\n",
    "\n",
    "print(\"-- English -- \")\n",
    "print(out[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e80e3d-d665-4d3b-b07b-aabd2b21ba9b",
   "metadata": {},
   "source": [
    "Build vocabulary. First loadi a spacy pretrained tokenizer for each language, split each sentence in its corresponing tokens and counting token frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aade840-bf77-45e8-8fb5-39fc1e8430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def build_vocab(filepath, tokenizer, min_freq=2):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "\n",
    "    return vocab(\n",
    "        counter, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"], min_freq=min_freq\n",
    "    )\n",
    "\n",
    "\n",
    "fr_vocab = build_vocab(train_filepaths[0], fr_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "# Set default token to <unk> if a new token is not in vocab\n",
    "fr_vocab.set_default_index(fr_vocab[\"<unk>\"])\n",
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "\n",
    "print(f'French, word to index mapping: fenêtre -> {fr_vocab[\"fenêtre\"]}')\n",
    "print(f'English, word to index mapping: window -> {en_vocab[\"window\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dec35-c705-47e0-949f-32ab5fe0c4db",
   "metadata": {},
   "source": [
    "Tokenize each sentence and wrap it in a Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c242f6-28c1-4dc4-bf10-2546f102f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "    raw_fr_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    data = []\n",
    "    for raw_fr, raw_en in zip(raw_fr_iter, raw_en_iter):\n",
    "        fr_tensor_ = torch.tensor(\n",
    "            [fr_vocab[token] for token in fr_tokenizer(raw_fr)], dtype=torch.long\n",
    "        )\n",
    "        en_tensor_ = torch.tensor(\n",
    "            [en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long\n",
    "        )\n",
    "        data.append((fr_tensor_, en_tensor_))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10246d-d341-42a5-8411-f5fae9a2ba99",
   "metadata": {},
   "source": [
    "Create dataset from tokenized samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494955dd-fc32-49f9-946b-ebb50f24a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = fr_vocab[\"<pad>\"]\n",
    "BOS_IDX = fr_vocab[\"<bos>\"]\n",
    "EOS_IDX = fr_vocab[\"<eos>\"]\n",
    "\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    fr_batch, en_batch = [], []\n",
    "    for fr_item, en_item in data_batch:\n",
    "        fr_batch.append(\n",
    "            torch.cat(\n",
    "                [torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0\n",
    "            )\n",
    "        )\n",
    "        en_batch.append(\n",
    "            torch.cat(\n",
    "                [torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0\n",
    "            )\n",
    "        )\n",
    "    fr_batch = pad_sequence(fr_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    return fr_batch, en_batch\n",
    "\n",
    "\n",
    "train_iter = DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch\n",
    ")\n",
    "valid_iter = DataLoader(\n",
    "    val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch\n",
    ")\n",
    "test_iter = DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
