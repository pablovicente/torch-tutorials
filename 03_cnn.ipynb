{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN in PyTorch\n",
    "\n",
    "References:\n",
    "- https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "- https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = './data'\n",
    "\n",
    "mnist_dataset = MNIST(root=path, download=True)\n",
    "\n",
    "# Get images and labels\n",
    "X = mnist_dataset.data\n",
    "y = mnist_dataset.targets\n",
    "\n",
    "# Normalize\n",
    "X = X / X.max()\n",
    "\n",
    "# X: (n_samples, h, w) -> (n_samples, n_channel, h, w)\n",
    "X = X.unsqueeze(dim=1)\n",
    "\n",
    "# Dimensionality\n",
    "#Â n_samples, n_samples = X.shape\n",
    "\n",
    "# Dimensionality\n",
    "n_samples, n_channels, n_h, n_w = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "imshow(torchvision.utils.make_grid(X[:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=2, out_channels=3, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1728, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check model\n",
    "\n",
    "Building our module and running through one batch could help debugging the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataLoader\n",
    "train_dataset = MNISTDataset(X, y)\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "\n",
    "model = CNN().to(device)\n",
    "y_hat = model(batch[0])\n",
    "\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, dataloader, model, loss_fn, optimizer, history=None):\n",
    "    \n",
    "    # Set train mode\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_batch = []\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print(f\"Epoch:{epoch} loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\", end='\\r')   \n",
    "            \n",
    "            train_loss_batch.append(loss)\n",
    "    \n",
    "    # End of epoch\n",
    "    print(f\"Epoch:{epoch} loss: {loss:>7f}  [{size:>5d}/{size:>5d}]\") \n",
    "    \n",
    "    # Save loss\n",
    "    if isinstance(history, defaultdict):\n",
    "        train_loss = sum(train_loss_batch)/len(train_loss_batch)\n",
    "        history['loss'].append(train_loss)\n",
    "            \n",
    "def test_loop(epoch, dataloader, model, loss_fn, history=None):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "    \n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_accuracy = correct / size\n",
    "    print(f\"Epoch:{epoch} Val accuracy: {(100*val_accuracy):>0.1f}%, Avg loss: {val_loss:>8f} \\n\")\n",
    "    \n",
    "    if isinstance(history, defaultdict):\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Keep track of model metrics\n",
    "history = defaultdict(list)\n",
    "    \n",
    "# Model hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Build DataLoader\n",
    "train_dataset = MNISTDataset(X, y)\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initalizer loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model for `num_epochs\n",
    "# For simplicity we are evaluating in the same dataset\n",
    "# You should always evaluate model performance on a separate holdout set\n",
    "for epoch in range(num_epochs):\n",
    "    train_loop(epoch, train_dl, model, loss_fn, optimizer, history)\n",
    "    test_loop(epoch, train_dl, model, loss_fn, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing ConvTranspose1d dimensionality\n",
    "\n",
    "Understanding how convolutions work for CNN and any layer you plan to use is key to develop any model. PyTorch includes a good summary of the inner workings of these layers.\n",
    "\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "\n",
    "In order to make a Conv layer work, we need to compute the dimensionality of certain parameters such as kernel_size, stride or padding. In this example, we look at ConvTranspose1d and build a few methods to help with the calculation:\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html\n",
    "\n",
    "\n",
    "Some parameters are left as default to work out the math of the formulas below:\n",
    "```\n",
    "groups=1\n",
    "dilation=1\n",
    "output_padding=0\n",
    "```\n",
    "\n",
    "Note that some combinations of parameters will result in float dims which are unfeasible dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data dim\n",
    "l_in = 28\n",
    "n_samples = 10\n",
    "\n",
    "# Conv1d parameter\n",
    "stride = 2\n",
    "padding = 2\n",
    "kernel_size = 3\n",
    "in_channels = 3\n",
    "out_channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l_out(l_in, stride, padding, kernel_size):\n",
    "    \n",
    "    return (l_in-1)*stride - 2*padding + (kernel_size-1) + 1\n",
    "\n",
    "l_out = compute_l_out(l_in, stride, padding, kernel_size)\n",
    "l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_padding(l_in, l_out, stride, kernel_size):\n",
    "    \"\"\"\n",
    "    Method to compute ConvTranspose1d where stride is known.\n",
    "    \"\"\"\n",
    "        \n",
    "    return ((l_in-1)*stride + kernel_size - l_out) / 2\n",
    "\n",
    "padding_val = compute_padding(l_in, l_out, stride, kernel_size)\n",
    "padding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stride(l_in, l_out, padding, kernel_size):\n",
    "    \"\"\"\n",
    "    Method to compute ConvTranspose1d where padding is known.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (l_out + 2*padding - kernel_size) / (l_in-1)\n",
    "\n",
    "stride_val = compute_stride(l_in, l_out, padding, kernel_size)\n",
    "stride_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dim(l_in, l_out, kernel_size, stride=None, padding=None):\n",
    "    \"\"\"\n",
    "    Method to compute ConvTranspose1d where either stride or padding is known.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert (stride is None) != (padding is None), \"Only one of the variables should be None\"\n",
    "    \n",
    "    if padding is None:\n",
    "        padding = compute_padding(l_in=l_in, l_out=l_out, stride=stride, kernel_size=kernel_size)\n",
    "    \n",
    "    if stride is None:\n",
    "        stride = compute_stride(l_in=l_in, l_out=l_out, padding=padding, kernel_size=kernel_size) \n",
    "        \n",
    "    dim = {'l_in':l_in, 'l_out':l_out, 'stride':stride, 'padding':padding, 'kernel_size':kernel_size}\n",
    "        \n",
    "    return dim\n",
    "\n",
    "dims = compute_dim(l_in, l_out, kernel_size=kernel_size, stride=None, padding=padding)\n",
    "print(dims)\n",
    "\n",
    "dims = compute_dim(l_in, l_out=53, kernel_size=kernel_size, stride=2, padding=None)\n",
    "print(dims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
